# Chinese-NewWordRecognition
中文新词发现/中文专业词库构建
##### 主要改进：
	1. 根据ngrams的设置，产生所有可能的词片段，并统计每个片段的频数。
	2. 根据频数计算每个片段的凝固度，不同长度的片段设置不同的阈值，根据阈值抛弃一定的片段。此时得到一个集合G，包含凝固度较高的片段。
	3. 使用集合G中的片段作为词库，切分文本，并统计切分后产生的片段的频数。根据阈值抛弃一定频数较低的片段。此时得到一个几个W，包含文本所有可能的词。注意，此时的W已经完全和G不一样了，W是大于G的，切分会产生很多新的片段。
	4. 使用集合G中的片段作为词库，切分文本，并统计切分后产生的片段的频数。根据阈值抛弃一定频数较低的片段。此时得到一个几个W，包含文本所有可能的词。注意，此时的W已经完全和G不一样了，W是大于G的，切分会产生很多新的片段。这里的切分规则也是特殊的：为了更大限度的保存可能的新词，采取宁放过，勿切错的原则，即：切分时，只要一个片段的一个 。
	这里的切分规则也是特殊的：为了更大限度的保存可能的新词，采取宁放过，勿切错的原则，即：切分时，只要一个片段的一个子串存在G中，这个片段就不切分，比如“我们俩”中的“我们”在G中，“我们俩”就不切分。采用这种原则，可以保存更多的分词可能。当然，这样做也可能导致更多的错误分词产生。后面可采用方法一定程度上删除这些错误。
	5. 删除可能的错误分词。重新遍历集合W，对于长度小于3的片段，检查其在不在集合G中，不在就抛弃。对于大于等于3 的片段，检查其所有可能的子串，如果其所有子串都在G中，则保留，否则抛弃。这么做是为了排除一些上一步产生的可能错误的分词。此时产生集合H，包含初步的可能分词。
	6. 边界熵计算。之所以把边界熵放到最后计算，是因为边界熵计算量过大，放在最后可以减少一定的计算量。统计集合H中所有片段的在文本中所有可能的左右字，计算每个片段的边界熵，根据阈值删除一定的片段。得到集合E，包含筛选出的文本中可能存在的词语。
	7. 发现领域新词。这里读取一个较大的通用词库C，包含可能的通用常用词。使用集合E减去集合C，最终得到该领域的专业词汇F。
